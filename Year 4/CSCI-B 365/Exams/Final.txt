Aman Patel
May 4, 2021
CSCI-B 365

Final Exam

Question 1

	1a.
		P(C = A | 1) = 0.01 / (0.01 + 0.77) = 0.0128
		P(C = B | 1) = 0.77 / (0.01 + 0.77) = 0.9872
		P(C = B | 1) > P(C = A | 1), therefore Bayes classification for feature 1 is B
		
		P(C = A | 2) = 0.53 / (0.53 + 0.13) = 0.8030
		P(C = B | 2) = 0.13 / (0.53 + 0.13) = 0.1970
		P(C = A | 2) > P(C = B | 2), therefore Bayes classification for feature 2 is A
		
		P(C = A | 3) = 0.46 / (0.46 + 0.10) = 0.8214 
		P(C = B | 3) = 0.10 / (0.46 + 0.10) = 0.1786
		P(C = A | 3) > P(C = B | 3), therefore Bayes classification for feature 3 is B
		
Question 2

	2a.
		X = matrix(c(.2, 0, .05, .2, .1, .1, 0, 0, .3, .1, .05, .1, .05, 0, .3, 0, 0, 0, 0, .2, 0, .05, 0, .1), byrow=TRUE, nrow = 6)
		colnames(X) = c("Mix 1", "Mix 2", "Mix 3", "Mix 4")
		rownames(X) = c("M&M's", "Licorice", "Gummy Worms", "Sweet Tarts", "Peanut Brittle", "Toffee")
		a = c(1, 2, 3, 4) # 1lb Mix 1, 2lbs Mix 2, 3lbs Mix 3, 4lbs Mix 4
		y = X %*% a
	
	2b. 
		y = c(0, 0, 1, 0, 0, 0)
		solve(t(X) %*% X, t(X) %*% y)
		
		Result:         [,1]
			Mix 1  2.2925190
			Mix 2  0.9092481
			Mix 3 -0.3787573
			Mix 4 -0.6843369
			
		Verify the solved matrix results in the desired quantity:
		X %*% solve(t(X) %*% X, t(X) %*% y)
		
		Result:                     [,1]
			M&M's           0.3026985499
			Licorice        0.3201767022
			Gummy Worms     0.6913089407
			Sweet Tarts     0.0009987516
			Peanut Brittle -0.1368673773
			Toffee         -0.0229712859
		
		This result is not exact as the question states. It is impossible to create this mixture for the following reasons:
			1. Peanut Brittle should have 0 lbs, so Mix 4 should not be used.
			2. Toffee should also have 0 lbs, so Mix 2 should not be used.
			3. Licorice should have 0 lbs, so Mix 1 should not be used.
			4. If only Mix 3 is used, there will be unwanted M&M's and Sweet Tarts in the final mixture
	
	2c.
		X2 = X[c("M&M's", "Licorice", "Gummy Worms", "Sweet Tarts"),]
		y = c(0, 0, 1, 0)
		solve(t(X2) %*% X2, t(X2) %*% y)
		
		Result:        [,1]
			Mix 1  10.43478
			Mix 2 -10.43478
			Mix 3  -1.73913
			Mix 4 -10.00000
			
		Verify the solved matrix results in the desired quantity:
		X %*% solve(t(X2) %*% X2, t(X2) %*% y)
		
		Result:                        [,1]
			M&M's           6.661338e-16
			Licorice        1.998401e-15
			Gummy Worms     1.000000e+00
			Sweet Tarts     1.110223e-16
			Peanut Brittle -2.000000e+00
			Toffee         -1.521739e+00
		
		The quantities for M&M's, Licorice, and Sweet Tarts are near 0 lbs and Gummy Worms is exactly 1 lb.
		However, the quantities for Peanut Brittle and Toffee are less than 0, which is impossible assuming we start
		with 0 lbs of all candies.
		
	2d.
		y = c(1, 1, 1, 1, 1, 1)
		solve(t(X) %*% X, t(X) %*% y)
		
		Result:        [,1]
			Mix 1 -3.639062
			Mix 2 14.532502
			Mix 3  2.505194
			Mix 4  3.737014
			
	2e.
		X %*% solve(t(X) %*% X, t(X) %*% y)
		
		Result:                 [,1]
			M&M's          0.8536445
			Licorice       0.8274273
			Gummy Worms    1.1553443
			Sweet Tarts    0.9985019
			Peanut Brittle 1.0514549
			Toffee         1.0344569
		
		All of the quantities are around 1 lb, but none are exactly 1 lb.

Question 3
	
	3a.
		P(T1 = 1 & T2 = 2 & T3 = 3) = # of instances / total = t[1, 2, 3] / 1000
		
	3b.
		P(T3 >= 3) = # of instances (T3 >= 3) / total
		#(T3 = 3) + #(T3 = 4) + #(T3 = 5) = sum(sum(t[, , 3])) + sum(sum(t[, , 4])) + sum(sum(t[, , 5]))
		P(T3 >= 3) = (sum(sum(t[, , 3])) + sum(sum(t[, , 4])) + sum(sum(t[, , 5]))) / 1000
		
		The sum function is used twice because the output of t[, , 3] is a 2D array
	
	3c. P(T3 = 3 | T1 = 1) = # of instances (T3 = 3 & T1 = 1) / # of instances (T1 = 1)
		#(T3 = 3 & T1 = 1) = sum(t[1,,3])
		#(T1 = 1) = sum(sum(t[1,,]))
		P(T3 = 3 | T1 = 1) = sum(t[1,,3]) / sum(sum(t[1,,]))
	
Question 4

	4a. 
		a_col = rep(1, 10)
		X_matrix = matrix(c(a_col, x, x^2, x^3))
		solve(t(X_matrix) %*% X_matrix, t(X_matrix) %*% y)
		
	4b.
		R's built-in regression function (solve) estimates values for the coefficients that minimize the SSE, or sum squared error.
		This is the sum of the differences between the predictions and the actual values squared.
		A smaller SSE is better than a larger SSE because SSE is a measure of how close the predictions are to the actual values.
	
	4c. 
		I would choose to use Model 3 (y = a + bx + cx^2) because it has a low SSE.
		Although Model 4's SSE is smaller than Model 3's, the difference is minute.
		This demonstrates that the fourth term in the polynomial does not have a significant effect on the model's accuracy.
		Excluding the fourth term will keep the model simpler and Model 3 will likely produce better accuracy on test data due to its lack of overfitting relative to Model 4.
	
Question 5
	
	5a.
		Ideally, the purity of class 1 or 2 would be 1 for a particular split point.
		The split point with the closest purity to 1 for a particular variable should be selected as the first split.
		
		maxPurity = 0
		maxVar = 0
		maxSplit = 0
		for (s in 1:numSplits) {
			purities = c(purity(1, s), purity(2, s))
			currentPurity = max(purities)
			currentVar = which.max(purities)
			if (currentPurity > maxPurity) {
				maxPurity = currentPurity
				maxVar = currentVar
				maxSplit = s
			}
		}
		print(maxVar)
		print(maxSplit)
		
	5b.
		node#   split var     split val      terminal      class
		1       x2            2.2            FALSE         1
		2       x1            1              FALSE         1
		3		x1            2.2            FALSE         2
		4                                    TRUE          2
		5                                    TRUE          1
		6                                    TRUE          1
		7                                    TRUE          2
		
Question 6

	6a.
		Iteration 1:
		
			m1 points: {(0, 3), (1, 3), (0, 4), (1, 4)}
			m2 points: {(0, 1), (1, 1), (0, 2), (1, 2)}
			
			m1 = ((0 + 1 + 0 + 1) / 4, (3 + 3 + 4 + 4) / 4) = (0.5, 3.5)
			m2 = ((0 + 1 + 0 + 1) / 4, (1 + 1 + 2 + 2) / 4) = (0.5, 1.5)
		
		Iteration 2:
			
			m1 points: {(0, 3), (1, 3), (0, 4), (1, 4)}
			m2 points: {(0, 1), (1, 1), (0, 2), (1, 2)}
			
			The clusters are unchanged, the algorithm has converged.
			
			Final clusters:
		
			m1 points: {(0, 3), (1, 3), (0, 4), (1, 4)}
			m2 points: {(0, 1), (1, 1), (0, 2), (1, 2)}
			
			Final centroids:
			
			m1 = (0.5, 3.5), m2 = (0.5, 1.5)
	
	6b. 
		Iteration 1:
			
			m1 points: {(0, 1), (0, 2), (0, 3), (0, 4)}
			m2 points: {(1, 1), (1, 2), (1, 3), (1, 4)}
			
			m1 = ((0 + 0 + 0 + 0) / 4, (1 + 2 + 3 + 4) / 4) = (0, 2.5)
			m2 = ((1 + 1 + 1 + 1) / 4, (1 + 2 + 3 + 4) / 4) = (1, 2.5)
			
		Iteration 2:
			
			m1 points: {(0, 1), (0, 2), (0, 3), (0, 4)}
			m2 points: {(1, 1), (1, 2), (1, 3), (1, 4)}
			
			The clusters are unchanged, the algorithm has converged.
			
			Final clusters:
			
			m1 points: {(0, 1), (0, 2), (0, 3), (0, 4)}
			m2 points: {(1, 1), (1, 2), (1, 3), (1, 4)}
			
			Final centroids:
			
			m1 = (0, 2.5), m2 = (1, 2.5)
			
	6c.
		Two K-means clustering results can be compared using their sum-squared errors.
		SSE is calculated by finding the sum of the distances between the centroids and the members of their respective cluster.
		
		Part a:
			
			m1SSE = (1/4 + 1/4) + (1/4 + 1/4) + (1/4 + 1/4) + (1/4 + 1/4) = 2
			m2SSE = (1/4 + 1/4) + (1/4 + 1/4) + (1/4 + 1/4) + (1/4 + 1/4) = 2
			
		Part b:
			
			m1SSE = (0 + 9/4) + (0 + 1/4) + (0 + 1/4) + (0 + 9/4) = 5
			m2SSE = (0 + 9/4) + (0 + 1/4) + (0 + 1/4) + (0 + 9/4) = 5
		
		The SSE for both clusters in Part b is higher than those in Part a. 
		This shows that the clustering in Part a is better than the clustering in Part b.
	
Question 7
	
	7a.
		a_col = rep(1, n)
		X_log = matrix(c(log2(a_col), x))
		res = solve(t(X_log) %*% X_log, t(X_log, y))
	
	7b.
		plot(x, res[1] * 2^(res[2] * x))
	
	7c.
		x2 = c(n+1:2n)
		points(x2, res[1] * 2^(res[2] * x2)) 