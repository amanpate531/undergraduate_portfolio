{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW4P6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2dCFLWT31G2"
      },
      "source": [
        "## Import all packages and functions needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGevpW6_3k54"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z73zUN8O0K6j"
      },
      "source": [
        "## Decision tree class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyk6Mbvkysc7"
      },
      "source": [
        "class dtree:\n",
        "\t\"\"\" A basic Decision Tree\"\"\"\n",
        "\n",
        "\tdef __init__(self): # include feature names specific to breast cancer dataset\n",
        "\t\t\"\"\" Constructor \"\"\"; self.featureNames = [\"radius_mean\", \"texture_mean\", \"perimeter_mean\", \"area_mean\", \"smoothness_mean\", \"compactness_mean\", \"concavity_mean\", \"concave_points_mean\", \"symmetry_mean\", \"fractal_dimension_mean\", \"radius_SE\", \"texture_SE\", \"perimeter_SE\", \"area_SE\", \"smoothness_SE\", \"compactness_SE\", \"concavity_SE\", \"concave_points_SE\", \"symmetry_SE\", \"fractal_dimension_SE\", \"radius_worst\", \"texture_worst\", \"perimeter_worst\", \"area_worst\", \"smoothness_worst\", \"compactness_worst\", \"concavity_worst\", \"concave_points_worst\", \"symmetry_worst\", \"fractal_dimension_worst\"]\n",
        "\n",
        "\tdef read_data(self, filename):\n",
        "\t\tfid = open(filename, \"r\")\n",
        "\t\tdata = []\n",
        "\t\td = []\n",
        "\t\tfor line in fid.readlines():\n",
        "\t\t\td.append(line.strip())\n",
        "\t\tfor d1 in d:\n",
        "\t\t\tdata.append(d1.split(\",\"))\n",
        "\t\tfid.close()\n",
        "\n",
        "\t\tself.classes = []\n",
        "\t\tfor d in range(len(data)): # change classes from strings to integers, remove first column\n",
        "\t\t\tself.classes.append(0 if data[d][1] == \"M\" else 1)\n",
        "\t\t\tdata[d] = data[d][2:]\n",
        "\n",
        "\t\treturn data, self.classes, self.featureNames\n",
        "\n",
        "\tdef classify(self, tree, datapoint): # final class is an integer, changed from string\n",
        "\t\tif type(tree) == int:\n",
        "\t\t\t# Have reached a leaf\n",
        "\t\t\treturn tree\n",
        "\t\telse:\n",
        "\t\t\ta = list(tree.keys())[0]\n",
        "\t\t\tfor i in range(len(self.featureNames)):\n",
        "\t\t\t\tif self.featureNames[i] == a:\n",
        "\t\t\t\t\tbreak\n",
        "\t\t\ttry:\n",
        "\t\t\t\tt = tree[a][datapoint[i]]\n",
        "\t\t\t\treturn self.classify(t, datapoint)\n",
        "\t\t\texcept:\n",
        "\t\t\t\treturn None\n",
        "\n",
        "\tdef classifyAll(self, tree, data):\n",
        "\t\tresults = []\n",
        "\t\tfor i in range(len(data)):\n",
        "\t\t\tresults.append(self.classify(tree, data[i]))\n",
        "\t\treturn results\n",
        "\n",
        "\tdef make_tree(self,\n",
        "\t\t\t\t  data,\n",
        "\t\t\t\t  classes,\n",
        "\t\t\t\t  featureNames,\n",
        "\t\t\t\t  maxlevel=-1,\n",
        "\t\t\t\t  level=0,\n",
        "\t\t\t\t  forest=0):\n",
        "\t\t\"\"\" The main function, which recursively constructs the tree\"\"\"\n",
        "\n",
        "\t\tnData = len(data)\n",
        "\t\tnFeatures = len(data[0])\n",
        "\n",
        "\t\ttry:\n",
        "\t\t\tself.featureNames\n",
        "\t\texcept:\n",
        "\t\t\tself.featureNames = featureNames\n",
        "\n",
        "\t\t# List the possible classes\n",
        "\t\tnewClasses = []\n",
        "\t\tfor aclass in classes:\n",
        "\t\t\tif newClasses.count(aclass) == 0:\n",
        "\t\t\t\tnewClasses.append(aclass)\n",
        "\n",
        "\t\t# Compute the default class (and total entropy)\n",
        "\t\tfrequency = np.zeros(len(newClasses))\n",
        "\n",
        "\t\ttotalEntropy = 0\n",
        "\t\tindex = 0\n",
        "\t\tfor aclass in newClasses:\n",
        "\t\t\tfrequency[index] = classes.count(aclass)\n",
        "\t\t\ttotalEntropy += self.calc_entropy(float(frequency[index]) / nData)\n",
        "\t\t\tindex += 1\n",
        "\n",
        "\t\tdefault = classes[np.argmax(frequency)]\n",
        "\n",
        "\t\tif nData == 0 or nFeatures == 0 or (maxlevel >= 0\n",
        "\t\t\t\t\t\t\t\t\t\t\tand level > maxlevel):\n",
        "\t\t\t# Have reached an empty branch\n",
        "\t\t\treturn default\n",
        "\t\telif classes.count(classes[0]) == nData:\n",
        "\t\t\t# Only 1 class remains\n",
        "\t\t\treturn classes[0]\n",
        "\t\telse:\n",
        "\n",
        "\t\t\t# Choose which feature is best\n",
        "\t\t\tgain = np.zeros(nFeatures)\n",
        "\t\t\tfeatureSet = list(range(nFeatures))\n",
        "\t\t\tif forest != 0:\n",
        "\t\t\t\tnp.random.shuffle(featureSet)\n",
        "\t\t\t\tfeatureSet = featureSet[0:forest]\n",
        "\t\t\tfor feature in featureSet:\n",
        "\t\t\t\tg = self.calc_info_gain(data, classes, feature)\n",
        "\t\t\t\tgain[feature] = totalEntropy - g\n",
        "\n",
        "\t\t\tbestFeature = np.argmax(gain)\n",
        "\t\t\ttree = {featureNames[bestFeature]: {}}\n",
        "\n",
        "\t\t\t# List the values that bestFeature can take\n",
        "\t\t\tvalues = []\n",
        "\t\t\tfor datapoint in data:\n",
        "\t\t\t\tif datapoint[feature] not in values:\n",
        "\t\t\t\t\tvalues.append(datapoint[bestFeature])\n",
        "\n",
        "\t\t\tfor value in values:\n",
        "\t\t\t\t# Find the datapoints with each feature value\n",
        "\t\t\t\tnewData = []\n",
        "\t\t\t\tnewClasses = []\n",
        "\t\t\t\tindex = 0\n",
        "\t\t\t\tfor datapoint in data:\n",
        "\t\t\t\t\tif datapoint[bestFeature] == value:\n",
        "\t\t\t\t\t\tif bestFeature == 0:\n",
        "\t\t\t\t\t\t\tnewdatapoint = datapoint[1:]\n",
        "\t\t\t\t\t\t\tnewNames = featureNames[1:]\n",
        "\t\t\t\t\t\telif bestFeature == nFeatures:\n",
        "\t\t\t\t\t\t\tnewdatapoint = datapoint[:-1]\n",
        "\t\t\t\t\t\t\tnewNames = featureNames[:-1]\n",
        "\t\t\t\t\t\telse:\n",
        "\t\t\t\t\t\t\tnewdatapoint = datapoint[:bestFeature]\n",
        "\t\t\t\t\t\t\t# newdatapoint.append(datapoint[bestFeature+1:])\n",
        "\t\t\t\t\t\t\tnewdatapoint = np.append(\n",
        "\t\t\t\t\t\t\t\tnewdatapoint, datapoint[bestFeature + 1:])\n",
        "\t\t\t\t\t\t\tnewNames = featureNames[:bestFeature]\n",
        "\t\t\t\t\t\t\t# newNames.append(featureNames[bestFeature+1:])\n",
        "\t\t\t\t\t\t\tnewNames = np.append(\n",
        "\t\t\t\t\t\t\t\tnewNames, featureNames[bestFeature + 1:])\n",
        "\t\t\t\t\t\tnewData.append(newdatapoint)\n",
        "\t\t\t\t\t\tnewClasses.append(classes[index])\n",
        "\t\t\t\t\tindex += 1\n",
        "\n",
        "\t\t\t\t# Now recurse to the next level\n",
        "\t\t\t\tsubtree = self.make_tree(newData, newClasses, newNames,\n",
        "\t\t\t\t\t\t\t\t\t\t maxlevel, level + 1, forest)\n",
        "\n",
        "\t\t\t\t# And on returning, add the subtree on to the tree\n",
        "\t\t\t\ttree[featureNames[bestFeature]][value] = subtree\n",
        "\n",
        "\t\t\treturn tree\n",
        "\n",
        "\tdef printTree(self, tree, name):\n",
        "\t\tif type(tree) == dict:\n",
        "\t\t\tprint(name, tree.keys()[0])\n",
        "\t\t\tfor item in tree.values()[0].keys():\n",
        "\t\t\t\tprint(name, item)\n",
        "\t\t\t\tself.printTree(tree.values()[0][item], name + \"\\t\")\n",
        "\t\telse:\n",
        "\t\t\tprint(name, \"\\t->\\t\", tree)\n",
        "\n",
        "\tdef calc_entropy(self, p):\n",
        "\t\tif p != 0:\n",
        "\t\t\treturn -p * np.log2(p)\n",
        "\t\telse:\n",
        "\t\t\treturn 0\n",
        "\n",
        "\tdef calc_info_gain(self, data, classes, feature):\n",
        "\n",
        "\t\t# Calculates the information gain based on entropy\n",
        "\t\tgain = 0\n",
        "\t\tnData = len(data)\n",
        "\n",
        "\t\t# List the values that feature can take\n",
        "\n",
        "\t\tvalues = []\n",
        "\t\tfor datapoint in data:\n",
        "\t\t\tif datapoint[feature] not in values:\n",
        "\t\t\t\tvalues.append(datapoint[feature])\n",
        "\n",
        "\t\tfeatureCounts = np.zeros(len(values))\n",
        "\t\tentropy = np.zeros(len(values))\n",
        "\t\tvalueIndex = 0\n",
        "\t\t# Find where those values appear in data[feature] and the corresponding class\n",
        "\t\tfor value in values:\n",
        "\t\t\tdataIndex = 0\n",
        "\t\t\tnewClasses = []\n",
        "\t\t\tfor datapoint in data:\n",
        "\t\t\t\tif datapoint[feature] == value:\n",
        "\t\t\t\t\tfeatureCounts[valueIndex] += 1\n",
        "\t\t\t\t\tnewClasses.append(classes[dataIndex])\n",
        "\t\t\t\tdataIndex += 1\n",
        "\n",
        "\t\t\t# Get the values in newClasses\n",
        "\t\t\tclassValues = []\n",
        "\t\t\tfor aclass in newClasses:\n",
        "\t\t\t\tif classValues.count(aclass) == 0:\n",
        "\t\t\t\t\tclassValues.append(aclass)\n",
        "\n",
        "\t\t\tclassCounts = np.zeros(len(classValues))\n",
        "\t\t\tclassIndex = 0\n",
        "\t\t\tfor classValue in classValues:\n",
        "\t\t\t\tfor aclass in newClasses:\n",
        "\t\t\t\t\tif aclass == classValue:\n",
        "\t\t\t\t\t\tclassCounts[classIndex] += 1\n",
        "\t\t\t\tclassIndex += 1\n",
        "\n",
        "\t\t\tfor classIndex in range(len(classValues)):\n",
        "\t\t\t\tentropy[valueIndex] += self.calc_entropy(\n",
        "\t\t\t\t\tfloat(classCounts[classIndex]) / np.sum(classCounts))\n",
        "\n",
        "\t\t\t# Computes the entropy\n",
        "\t\t\tgain = gain + float(\n",
        "\t\t\t\tfeatureCounts[valueIndex]) / nData * entropy[valueIndex]\n",
        "\t\t\tvalueIndex += 1\n",
        "\t\treturn gain"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwD3SlLJyuSA"
      },
      "source": [
        "## Testing decision tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAJKENqOypjU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f3c0d26-4348-43b1-8d0c-1ae1f476f474"
      },
      "source": [
        "data, classes, names = dtree().read_data('wdbc.data')\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, classes, test_size=0.5, random_state = 3)\n",
        "tree = dtree().make_tree(X_test, y_test, names, forest=0)\n",
        "res = dtree().classifyAll(tree, X_test)\n",
        "count = 0\n",
        "for i in range(len(res)):\n",
        "  if res[i] == y_test[i]:\n",
        "    count += 1\n",
        "print(\"Accuracy:\", count / len(res))\n",
        "#print(1,2,3)"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9859649122807017\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2W2knOkYC8aj"
      },
      "source": [
        "## Results\n",
        "\n",
        "The random forest predictive model performs slightly better than the Multi-Layer Perceptron model implemented in Project 2. This is most likely due to bootstrapping in the random forest model, which reduces overfitting and improves the stability of decision trees. The model can be improved by tweaking the number of trees in the model and the number of features used."
      ]
    }
  ]
}