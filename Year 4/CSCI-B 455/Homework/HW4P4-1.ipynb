{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW4P4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2dCFLWT31G2"
      },
      "source": [
        "## Import all packages and functions needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGevpW6_3k54"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z73zUN8O0K6j"
      },
      "source": [
        "## Decision tree class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyk6Mbvkysc7"
      },
      "source": [
        "class dtree:\n",
        "\t\"\"\" A basic Decision Tree\"\"\"\n",
        "\n",
        "\tdef __init__(self):\n",
        "\t\t\"\"\" Constructor \"\"\"\n",
        "\n",
        "\tdef read_data(self, filename):\n",
        "\t\tfid = open(filename, \"r\")\n",
        "\t\tdata = []\n",
        "\t\td = []\n",
        "\t\tfor line in fid.readlines():\n",
        "\t\t\td.append(line.strip())\n",
        "\t\tfor d1 in d:\n",
        "\t\t\tdata.append(d1.split(\",\"))\n",
        "\t\tfid.close()\n",
        "\n",
        "\t\tself.featureNames = data[0]\n",
        "\t\tself.featureNames = self.featureNames[:-1]\n",
        "\t\tdata = data[1:]\n",
        "\t\tself.classes = []\n",
        "\t\tfor d in range(len(data)):\n",
        "\t\t\tself.classes.append(data[d][-1])\n",
        "\t\t\tdata[d] = data[d][:-1]\n",
        "\n",
        "\t\treturn data, self.classes, self.featureNames\n",
        "\n",
        "\tdef classify(self, tree, datapoint):\n",
        "\n",
        "\t\tif type(tree) == type(\"string\"):\n",
        "\t\t\t# Have reached a leaf\n",
        "\t\t\treturn tree\n",
        "\t\telse:\n",
        "\t\t\ta = list(tree.keys())[0]\n",
        "\t\t\tfor i in range(len(self.featureNames)):\n",
        "\t\t\t\tif self.featureNames[i] == a:\n",
        "\t\t\t\t\tbreak\n",
        "\t\t\ttry:\n",
        "\t\t\t\tt = tree[a][datapoint[i]]\n",
        "\t\t\t\treturn self.classify(t, datapoint)\n",
        "\t\t\texcept:\n",
        "\t\t\t\treturn None\n",
        "\n",
        "\tdef classifyAll(self, tree, data):\n",
        "\t\tresults = []\n",
        "\t\tfor i in range(len(data)):\n",
        "\t\t\tresults.append(self.classify(tree, data[i]))\n",
        "\t\treturn results\n",
        "\n",
        "\tdef make_tree(self,\n",
        "\t\t\t\t  data,\n",
        "\t\t\t\t  classes,\n",
        "\t\t\t\t  featureNames,\n",
        "\t\t\t\t  maxlevel=-1,\n",
        "\t\t\t\t  level=0,\n",
        "\t\t\t\t  forest=0):\n",
        "\t\t\"\"\" The main function, which recursively constructs the tree\"\"\"\n",
        "\n",
        "\t\tnData = len(data)\n",
        "\t\tnFeatures = len(data[0])\n",
        "\n",
        "\t\ttry:\n",
        "\t\t\tself.featureNames\n",
        "\t\texcept:\n",
        "\t\t\tself.featureNames = featureNames\n",
        "\n",
        "\t\t# List the possible classes\n",
        "\t\tnewClasses = []\n",
        "\t\tfor aclass in classes:\n",
        "\t\t\tif newClasses.count(aclass) == 0:\n",
        "\t\t\t\tnewClasses.append(aclass)\n",
        "\n",
        "\t\t# Compute the default class (and total entropy)\n",
        "\t\tfrequency = np.zeros(len(newClasses))\n",
        "\n",
        "\t\ttotalEntropy = 0\n",
        "\t\tindex = 0\n",
        "\t\tfor aclass in newClasses:\n",
        "\t\t\tfrequency[index] = classes.count(aclass)\n",
        "\t\t\ttotalEntropy += self.calc_entropy(float(frequency[index]) / nData)\n",
        "\t\t\tindex += 1\n",
        "\n",
        "\t\tdefault = classes[np.argmax(frequency)]\n",
        "\n",
        "\t\tif nData == 0 or nFeatures == 0 or (maxlevel >= 0\n",
        "\t\t\t\t\t\t\t\t\t\t\tand level > maxlevel):\n",
        "\t\t\t# Have reached an empty branch\n",
        "\t\t\treturn default\n",
        "\t\telif classes.count(classes[0]) == nData:\n",
        "\t\t\t# Only 1 class remains\n",
        "\t\t\treturn classes[0]\n",
        "\t\telse:\n",
        "\n",
        "\t\t\t# Choose which feature is best\n",
        "\t\t\tgain = np.zeros(nFeatures)\n",
        "\t\t\tfeatureSet = list(range(nFeatures))\n",
        "\t\t\tif forest != 0:\n",
        "\t\t\t\tnp.random.shuffle(featureSet)\n",
        "\t\t\t\tfeatureSet = featureSet[0:forest]\n",
        "\t\t\tfor feature in featureSet:\n",
        "\t\t\t\tg = self.calc_info_gain(data, classes, feature)\n",
        "\t\t\t\tgain[feature] = 1 - g\n",
        "\n",
        "\t\t\tbestFeature = np.argmax(gain)\n",
        "\t\t\ttree = {featureNames[bestFeature]: {}}\n",
        "\n",
        "\t\t\t# List the values that bestFeature can take\n",
        "\t\t\tvalues = []\n",
        "\t\t\tfor datapoint in data:\n",
        "\t\t\t\tif datapoint[feature] not in values:\n",
        "\t\t\t\t\tvalues.append(datapoint[bestFeature])\n",
        "\n",
        "\t\t\tfor value in values:\n",
        "\t\t\t\t# Find the datapoints with each feature value\n",
        "\t\t\t\tnewData = []\n",
        "\t\t\t\tnewClasses = []\n",
        "\t\t\t\tindex = 0\n",
        "\t\t\t\tfor datapoint in data:\n",
        "\t\t\t\t\tif datapoint[bestFeature] == value:\n",
        "\t\t\t\t\t\tif bestFeature == 0:\n",
        "\t\t\t\t\t\t\tnewdatapoint = datapoint[1:]\n",
        "\t\t\t\t\t\t\tnewNames = featureNames[1:]\n",
        "\t\t\t\t\t\telif bestFeature == nFeatures:\n",
        "\t\t\t\t\t\t\tnewdatapoint = datapoint[:-1]\n",
        "\t\t\t\t\t\t\tnewNames = featureNames[:-1]\n",
        "\t\t\t\t\t\telse:\n",
        "\t\t\t\t\t\t\tnewdatapoint = datapoint[:bestFeature]\n",
        "\t\t\t\t\t\t\t# newdatapoint.append(datapoint[bestFeature+1:])\n",
        "\t\t\t\t\t\t\tnewdatapoint = np.append(\n",
        "\t\t\t\t\t\t\t\tnewdatapoint, datapoint[bestFeature + 1:])\n",
        "\t\t\t\t\t\t\tnewNames = featureNames[:bestFeature]\n",
        "\t\t\t\t\t\t\t# newNames.append(featureNames[bestFeature+1:])\n",
        "\t\t\t\t\t\t\tnewNames = np.append(\n",
        "\t\t\t\t\t\t\t\tnewNames, featureNames[bestFeature + 1:])\n",
        "\t\t\t\t\t\tnewData.append(newdatapoint)\n",
        "\t\t\t\t\t\tnewClasses.append(classes[index])\n",
        "\t\t\t\t\tindex += 1\n",
        "\n",
        "\t\t\t\t# Now recurse to the next level\n",
        "\t\t\t\tsubtree = self.make_tree(newData, newClasses, newNames,\n",
        "\t\t\t\t\t\t\t\t\t\t maxlevel, level + 1, forest)\n",
        "\n",
        "\t\t\t\t# And on returning, add the subtree on to the tree\n",
        "\t\t\t\ttree[featureNames[bestFeature]][value] = subtree\n",
        "\n",
        "\t\t\treturn tree\n",
        "\n",
        "\tdef printTree(self, tree, name):\n",
        "\t\tif type(tree) == dict:\n",
        "\t\t\tprint(name, tree.keys()[0])\n",
        "\t\t\tfor item in tree.values()[0].keys():\n",
        "\t\t\t\tprint(name, item)\n",
        "\t\t\t\tself.printTree(tree.values()[0][item], name + \"\\t\")\n",
        "\t\telse:\n",
        "\t\t\tprint(name, \"\\t->\\t\", tree)\n",
        "\n",
        "\tdef calc_entropy(self, p):\n",
        "\t\tif p != 0:\n",
        "\t\t\treturn p ** 2\n",
        "\t\telse:\n",
        "\t\t\treturn 0\n",
        "\n",
        "\tdef calc_info_gain(self, data, classes, feature):\n",
        "\n",
        "\t\t# Calculates the information gain based on entropy\n",
        "\t\tgain = 0\n",
        "\t\tnData = len(data)\n",
        "\n",
        "\t\t# List the values that feature can take\n",
        "\n",
        "\t\tvalues = []\n",
        "\t\tfor datapoint in data:\n",
        "\t\t\tif datapoint[feature] not in values:\n",
        "\t\t\t\tvalues.append(datapoint[feature])\n",
        "\n",
        "\t\tfeatureCounts = np.zeros(len(values))\n",
        "\t\tentropy = np.zeros(len(values))\n",
        "\t\tvalueIndex = 0\n",
        "\t\t# Find where those values appear in data[feature] and the corresponding class\n",
        "\t\tfor value in values:\n",
        "\t\t\tdataIndex = 0\n",
        "\t\t\tnewClasses = []\n",
        "\t\t\tfor datapoint in data:\n",
        "\t\t\t\tif datapoint[feature] == value:\n",
        "\t\t\t\t\tfeatureCounts[valueIndex] += 1\n",
        "\t\t\t\t\tnewClasses.append(classes[dataIndex])\n",
        "\t\t\t\tdataIndex += 1\n",
        "\n",
        "\t\t\t# Get the values in newClasses\n",
        "\t\t\tclassValues = []\n",
        "\t\t\tfor aclass in newClasses:\n",
        "\t\t\t\tif classValues.count(aclass) == 0:\n",
        "\t\t\t\t\tclassValues.append(aclass)\n",
        "\n",
        "\t\t\tclassCounts = np.zeros(len(classValues))\n",
        "\t\t\tclassIndex = 0\n",
        "\t\t\tfor classValue in classValues:\n",
        "\t\t\t\tfor aclass in newClasses:\n",
        "\t\t\t\t\tif aclass == classValue:\n",
        "\t\t\t\t\t\tclassCounts[classIndex] += 1\n",
        "\t\t\t\tclassIndex += 1\n",
        "\n",
        "\t\t\tfor classIndex in range(len(classValues)):\n",
        "\t\t\t\tentropy[valueIndex] += self.calc_entropy(\n",
        "\t\t\t\t\tfloat(classCounts[classIndex]) / np.sum(classCounts))\n",
        "\n",
        "\t\t\t# Computes the entropy\n",
        "\t\t\tgain = gain + float(\n",
        "\t\t\t\tfeatureCounts[valueIndex]) / nData * entropy[valueIndex]\n",
        "\t\t\tvalueIndex += 1\n",
        "\t\treturn gain"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwD3SlLJyuSA"
      },
      "source": [
        "## Testing decision tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAJKENqOypjU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b1c2926-60b9-47dc-853d-dad69eb2bd8a"
      },
      "source": [
        "data = [[0, 0], [1, 0], [0, 1], [1, 1]]\n",
        "classes = [0, 1, 1, 1]\n",
        "names = ['x1', 'x2']\n",
        "tree = dtree().make_tree(data, classes, names)\n",
        "#print(1,2,3)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'x1': {0: {'x2': {0: 0, 1: 1}}, 1: 1}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3v0WeRrC_87"
      },
      "source": [
        "## Results\n",
        "\n",
        "The entropy and information gain calculations were altered to conform to Gini Impurity. The entropy was changed from `-p * log_2(p)` to `p ** 2`. Instead of calculating total entropy for use in information gain, the total entropy is set to 1 for all information gain calculations."
      ]
    }
  ]
}