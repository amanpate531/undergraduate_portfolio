{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oagvR20NeVU"
      },
      "source": [
        "#Project 5 - Predicting the Likelihood of Graduate Admission for Indian Students Using ML Models\n",
        "Aman Patel\n",
        "\n",
        "CSCI-B 455\n",
        "\n",
        "April 25, 2021"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMHb9xIkOIvY"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GshHx1DOMoT"
      },
      "source": [
        "## Problem Statement\n",
        "The goal for this project was to create three distinct ML models to predict the likelihood that an Indian student would be admitted to graduate school."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChIjFWutOXhi"
      },
      "source": [
        "## Data\n",
        "The dataset used for this project, \"Graduate Admission 2\", was collected from Mohan S Acharya on Kaggle. It contained seven academic attributes for 500 students including GRE, TOEFL, CGPA, and more. The data can be found at https://www.kaggle.com/mohansacharya/graduate-admissions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOXugvBAQOrr"
      },
      "source": [
        "## Model Parameters\n",
        "The first model used was a default LinearRegression. This is a simple model that aims to minimize the SSE between the data and the prediction.\n",
        "\n",
        "The second model used was a DecisionTreeRegressor. This was also mostly using default settings, but the max depth of the tree was changed to minimize overfitting while maintaining a level of complexity.\n",
        "\n",
        "The third model used was a default SGDRegressor. This model is trained using Stochastic Gradient Descent, which gradually oscillates about local maxima until convergence is reached. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKFjvQu7RZur"
      },
      "source": [
        "# **Code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkCjBQVHGbQ7",
        "outputId": "ce165204-28af-4fda-cf61-466f1c57603e"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import sklearn\n",
        "\n",
        "# data file processed and split into features and targets\n",
        "f = open('Admission_Predict_Ver1.1.csv')\n",
        "f.readline()\n",
        "data = np.loadtxt(f,delimiter=',')\n",
        "X = data[:, 1:-1]\n",
        "y = data[:, -1]\n",
        "\n",
        "# All features normalized to keep training even between features\n",
        "X = preprocessing.StandardScaler().fit_transform(X)\n",
        "\n",
        "# Encodes the targets for use in multi-class classification (affects SGDRegressor)\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "y = encoder.fit_transform(y)\n",
        "\n",
        "# Split training and testing data randomly using 40% test and 60% train\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 1)\n",
        "\n",
        "# Initialize the models\n",
        "model1 = LinearRegression()\n",
        "model2 = DecisionTreeRegressor(max_depth = 3)\n",
        "model3 = SGDRegressor()\n",
        "\n",
        "# Train the models\n",
        "model1.fit(X_train, y_train)\n",
        "model2.fit(X_train, y_train)\n",
        "model3.fit(X_train, y_train)\n",
        "\n",
        "# Perform 5-fold cross validation to test the models\n",
        "model1_scores = cross_val_score(model1, X_test, y_test, scoring='r2', cv=5)\n",
        "model2_scores = cross_val_score(model2, X_test, y_test, scoring='r2', cv=5)\n",
        "model3_scores = cross_val_score(model3, X_test, y_test, scoring='r2', cv=5)\n",
        "\n",
        "# Prints the cross-validation scores and the average score\n",
        "print(model1_scores, np.mean(model1_scores))\n",
        "print(model2_scores, np.mean(model2_scores))\n",
        "print(model3_scores, np.mean(model3_scores))\n"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.66967059 0.87540069 0.83332021 0.84154644 0.81193494] 0.8063745732971566\n",
            "[0.46377999 0.79293067 0.71338152 0.77872167 0.70073842] 0.6899104523122412\n",
            "[0.66643891 0.88152298 0.8346214  0.83888757 0.80987084] 0.8062683401492106\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy5VmHHeStRX"
      },
      "source": [
        "# **Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mST704lbSwxZ"
      },
      "source": [
        "As shown in the printed results, the LinearRegression and SGDRegressor models performed well, with similar scores in all five validations. The DecisionTreeRegressor model had noticeably lower cross-validation scores than the other models (80.6% vs. 69%)\n",
        "\n",
        "Interestingly, the first cross-validation score for each model was significantly lower than the following four. The reason why this occurs is unknown, but further research can uncover it.\n",
        "\n",
        "The models can be improved by tweaking the parameters, which can reduce overfitting and decrease training time. "
      ]
    }
  ]
}